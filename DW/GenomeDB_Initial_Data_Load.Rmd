---
title: "GenomeDB_Initial_Data_Load"
author: "Nick Burns"
date: "5 August 2016"
output: html_document
---

# GenomeDB  
## Initial data load  

**Objective:** to load GTEx metadata (-> dim_tissue), GTEx expression data (-> fact_expression), GTEx gene summary (-> dim_coordinates, gim_gene) and GTEx QTL data (-> fact_qtl, dim_coordinate).  

We should be able to do this in 6 steps:  

  1. Load GTEx data into staging tables  
  2. Manually add an entry for GTEx into dim_dataset  
  3. From stage.meta, populate dim_tissue  
  4. From stage.gene, populate dim_coordinate and dim_gene  
  5. From stage.expression, populate fact_expression  
  6. From stage.qtl, update dim_coordinate then, populate fact_qtl  
  
For this initial load, we will simply step through this process. In the future we will need to consider robust methods for updating data / entering new datasets. 

### Step 0: setup environment  

Setup ODBC connection and confirm that it is working.  

```{r}
library(RODBC)
library(data.table)
data_dir <- "G:\\Datasets\\GenomeDBData\\"

conn <- odbcConnect("R-SQL")
sqlQuery(conn, "use GenomeDB;")
sqlQuery(conn, "select name from sys.tables;")
odbcClose(conn)
```

### Step 1: populate staging area  

Load gtex_metadata, gtex_genes, gtex_expression into staging areas. (QTL I will do later)

```{r}
input_files <- list(
    meta = "gtex_metadata.csv",
    gene = "gtex_genes.csv"
)

for (i in 1:length(input_files)) {
    
    start <- Sys.time()
    table_ <- names(input_files)[i]
    file_ <- paste0(data_dir, input_files[[i]], sep = "")
    
    # note: all files have headers, so -F2 will skip the header row
    #       had to remove quotes around strings, otherwise data not imported!!!
    cmd_ <- sprintf("bcp GenomeDB.stage.%s in %s -T -S ORBITAL\\SQLDEVBOX -c -r\\n -t, -F2",
                    table_, file_)
        
    print(cmd_)
    system(cmd_)
    
    print(sprintf("Loading %s took: %s", table_, Sys.time() - start))
}
```

NOTE: unable to populate the expression staging table this way - the file is too large. BCP failed, bulk insert failed. Am currently trying SQL Server data import / export wizard. Slow - but it is inserting rows. The only concern is whether I run out of memory...


The next few steps I ended up doing directly in SQL. See GenomeDB_InitData.sql
### Step 2: Update dim_dataset  
### Step 3: populate dim_tissue   
### Step 4: unstage stage.gene into dim_coordinate and dim_gene  
### Step 5: unstage stage.expression into fact_expression  

### Step 6: unstage stage.qtl into dim_coordinate and fact_qtl   

Below, we setup a few helper functions to help parse the QTL data files, then load them one by one.

```{r}
strip_suffix <- function(x, suffix = "_Analysis_cis-eQTLs.txt") {
    tissue <- substr(x, start = 1, stop = nchar(x) - nchar(suffix))
    return (tissue)
}
# map_tissue()
# removes underscores and extracts the first 16 characters from the tissue
# 16 characters is enough to uniquely map to dim_tisse.smtsd
map_tissue <- function(suffix, db = dim_tissue) {
    suffix <- substr(gsub("_", " ", suffix), 1, 16)
    idx <- grep(suffix, db$stub)
    return (db[idx, "tissue_id"])
}
reset_staging <- function (conn) {
    sqlQuery(conn, "truncate table GenomeDB.stage.qtl;")
}
bulk_load <- function (qtl_file) {
    
    cmd_ <- sprintf("bcp GenomeDB.stage.qtl in %s -T -S ORBITAL\\SQLDEVBOX -c -r\\n -t\\t -b 100000",
                    qtl_file)
    system(cmd_)
}
unstage <- function(conn, tissue_id, source_id) {
    
    print("Unstaging QTL data...")
    query <- sprintf("GenomeDB.dbo.unstage_qtl @tissue = %s, @datasource = %s;", 
                     tissue_id, source_id)
    sqlQuery(conn, query)
}
```

NOTE: had to convert all of hte qtl files using u2d in bash (this took hours):

```
$ for file in *eQTLs.txt
> do
>    echo $file
>    u2d $file 
> done;
```

Setup database connection and global variables:
```{r}
setwd("G:/Datasets/GenomeDBData/filtered_sets/filtered_sets/")
library(RODBC)

qtl_file_pattern <- "eQTLs.txt"
qtl_files <- list.files(".", pattern = qtl_file_pattern)

conn <- odbcConnect("R-SQL")
sqlQuery(conn, "use GenomeDB;")
dim_tissue <- sqlQuery(conn, "select *, LEFT(smtsd, 20) as 'stub' from GenomeDB.dbo.dim_tissue;")
dim_tissue$stub <- gsub(" - ", " ", dim_tissue$stub)
dim_tissue$stub <- gsub(" \\(", " ", dim_tissue$stub)
odbcClose(conn)

for (qtl_ in qtl_files) {
    
    print(sprintf("--------    %s    --------", qtl_))
    print("")
    lcl_start <- Sys.time()
    
    # extract tissue name
    # match to tissue id via dim_tissue$stub
    tissue <- strip_suffix(qtl_)
    tissue_id <- map_tissue(tissue, db = dim_tissue)
    
    # load qtl data
    conn <- odbcConnect("R-SQL")
    reset_staging(conn)

    bulk_load(paste0(getwd(), "/", qtl_, sep = ""))

    unstage(conn, tissue_id, 1)
    odbcClose(conn)
    
    print("")
    print(sprintf("Time for %s: %s", qtl_, Sys.time() - lcl_start))
    print("")
    print("")
    
}

```


### GWAS data  

For now, I have decided to load this manually using SQL Server Data Import / Export tool. I just want to get this data in, then I can iterate on this and figure out how to possibly streamline it. Loading the following data initially:

  - Kottgen (urate and gout)  
  - Locke and Shungin obesity  
  - Diagram consortium's diabetes  
  - Tanya's T2D  
  
